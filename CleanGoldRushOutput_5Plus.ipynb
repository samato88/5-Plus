{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samato88/5-Plus/blob/main/CleanGoldRushOutput_5Plus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQz2ybguYjQp"
      },
      "source": [
        "Take a GR output file and extract OCNs, and EAST OCLC API and Hathi Counts, States Retained In, Held In States, Zone, Retained By Symbol, Held By Symbols\n",
        "and Region (risk)\n",
        "\n",
        "Takes 10 hours to run full file - need to optimize!!!\n",
        "\n",
        "TO DO:\n",
        "*   With WC_DF - could do changes in slices rather than iterrows and it would be faster. Currently I think it takes about 5 min to work through\n",
        "\n",
        "\n",
        "DONE (I think):\n",
        "*   Test by library = location of ocn - I think this is done\n",
        "*   Rework Directories - 5+ rather than generic\n",
        "*   Itertuples rather than iterrows -  wccounts df,\n",
        "*   Fix up NYU OCNs - made a secondary file for lookups of MMSID to OCN based on 079s and put two lines of test data in test file \"EAST5+.test.csv\" has the last item, 5001, as an NYU that lacks an OCN.\n",
        "Code looks for both no 035 and no ocn in 035 ***THIS NOT WORKING YET***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JOkbpDPLBUcn"
      },
      "outputs": [],
      "source": [
        "#@title check memory\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title code to skip cells - %%skip\n",
        "\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def skip(line, cell):\n",
        "    return"
      ],
      "metadata": {
        "id": "-qTtOJIWS1EK",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDWDWJqq7SsX"
      },
      "outputs": [],
      "source": [
        "#@title import google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.flush_and_unmount()  # Unmount existing mount - trying since not reading new files\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSz4YlnGL58z"
      },
      "outputs": [],
      "source": [
        "#@title install and use ipdb debugger, install xl packages\n",
        "!pip install ipdb\n",
        "!pip install xlsxwriter\n",
        "!pip install openpyxl\n",
        "\n",
        "import ipdb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybAC9eGqZM-t"
      },
      "outputs": [],
      "source": [
        "#@title imports\n",
        "import os\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    sys.path.append('/content/drive/MyDrive/Colab Notebooks/Includes/')\n",
        "\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "import csv\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "from MarkSuspectRegEx import title, publisher\n",
        "from Parse035FieldRegEx import parse_035, mapPascal # should put pascal somewhere more logical, but this works...\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nrh7ZTNcZoFf",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Comments on input / output fields\n",
        "# some code written by ChatGPT, May 2023\n",
        "# USAGE: python3 CleanGoldRushOutput - MUST UPDATE COUNTS, DIR and FILE for what you want\n",
        "\n",
        "# To DO: might want to add column of Zones Retained In, currently doing those counts in Allocate2023\n",
        "\n",
        "# Description:\n",
        "# given a Gold Rush xlsx file, a file of EAST counts(name defined in dir and file variables below), do the following:\n",
        "# Parse the 035 field and add a new column for OCN\n",
        "# Check OCN to see if we have or need an EAST count\n",
        "# Create a new columns for EAST Count, State of holding, State of retentions, Suspect pub or title, Census zone and regionfor risk modeling\n",
        "# Create a new sheet for rows missing an OCN, remove those rows from Sheet 1\n",
        "# Create a new sheet for OCNs lacking an EAST count, also save these as a tsv to feed to API\n",
        "# Create a new tsv files of Sheet 1 - \"ForAllFile_\" + sheetname.tsv\n",
        "\n",
        "## See around line 220 for splitting if combined library file - name/ctrl number\n",
        "#\n",
        "# Creates 4 new files - dist_modified, ForAllFile and ONCs4EASTAPI, ONCs4HOLDINGSAPI\n",
        "### Does not take into account # of EAST holdings, e.g. removing higher than 5, that is done in allocation.\n",
        "\n",
        "  #   0\tA Title(245$a)\n",
        "  # 1 B Title(245$b)\n",
        "  #   2\tC Author\n",
        "  #   3\tD Control Number(001)\n",
        "  #   4\tE Library\n",
        "  #   5\tF Branch\n",
        "  #   6\tG ISBN\n",
        "  #   7\tH ISSN\n",
        "  #   8\tI Publisher Name\n",
        "  #   9\tJ Publisher Number\n",
        "  #   10\tK LC Call Number\n",
        "  #   11\tL Dewey Call Number\n",
        "  #   12\tM SuDocs Call Number\n",
        "  #   13\tN Report Number\n",
        "  #   14\tO General Report Number\n",
        "  #   15\tP Edition\n",
        "  #   16\tQ Action Note(583$f)\n",
        "  #   17\tR Action Note(583$h)\n",
        "  #   18\tS Publication Date\n",
        "  #   19\tT Language\n",
        "  #   20\tU MatchKey <-- required to have a matchkey, using for cuml circ I think\n",
        "  #   21\tV Circ. Count <-- only in reports with circ => move this to position 21 if exists\n",
        "  #   22\tW 035 field\n",
        "  # 23  X Juvenile Lit.\n",
        "  # 24  Y Gov Docs(086$a)field\n",
        "\n",
        "\n",
        "# want output to be:\n",
        "# A\t0\tTitle\n",
        "# B\t1\tAuthor\n",
        "# C\t2\tLibrary\n",
        "# D\t3\tControl Field - 001\n",
        "# E\t4\tISBN\n",
        "# F\t5\tPublisher Name\n",
        "# G\t6\tLC\n",
        "# H 7 LC Class\n",
        "# I\t8\tEdition\n",
        "# J\t9\tPublication Date\n",
        "# K\t10\tLanguage\n",
        "# L\t11\tMatchKey\n",
        "# M\t12\t035 field\n",
        "# N\t13\tOCN\n",
        "# O\t14\tEAST Counts\n",
        "# Q 15  WorldCat Holdings\n",
        "# R\t16\tFrom Sheet * NO OCN Output outputs up to Q\n",
        "# S\t17\tCirc. Count - nowadays blank...\n",
        "# T 18  Juvenile Lit.\n",
        "# U 19  HathiTrust\n",
        "# V 20  Doc: 086\n",
        "# W 21  Suspect\n",
        "# X 22  States Retained In\n",
        "# Y 23  Held In States\n",
        "# Z 24  Zone\n",
        "# AA 25  Retained By Symbol\n",
        "# AB 26 Held By Symbols\n",
        "# AC 27 Region\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CkX7bi-t_i5N"
      },
      "outputs": [],
      "source": [
        "#@title Define files and environment\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "now = datetime.now()\n",
        "pacific_tz = ZoneInfo('America/Los_Angeles')\n",
        "now_pacific = now.astimezone(pacific_tz)\n",
        "print(f\"Starting at: {now_pacific.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "counter = 0\n",
        "\n",
        "############# UPDATE THESE FOR FILE YOU ARE RUNNING #########################\n",
        "test = False\n",
        "cloud = True\n",
        "extension = \"txt\" # are we dealing with an excel input or large csv file that excel can't handle (txt|, xlsx or csv,)\n",
        "#########\n",
        "\n",
        "if cloud:\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/Includes/\"\n",
        "    inputfiledir = \"/content/drive/MyDrive/Colab Notebooks/5+/\"\n",
        "    libmetadata = \"/content/drive/MyDrive/Colab Notebooks/Includes/Member_Metadata.xlsx\"; # state, % retained\n",
        "\n",
        "else:\n",
        "    path = \"/Users/samato/Dropbox/EAST/Gold_Rush/Comparisons/Scripts/\"\n",
        "    inputfiledir = \"/Users/samato/Dropbox/EAST/Comparisons/PALNI/\" ;\n",
        "\n",
        "if test:  #### - files for testing/development - ####\n",
        "    feedbacklinecount = 50\n",
        "    eastcounts = path + \"KNOWN-EAST-HOLDINGS.test.csv\" ; #\n",
        "    wccounts = path + \"KNOWN-WC-HOLDINGS.test.csv\"\n",
        "    htcounts = path + \"KNOWN-HATHI-HOLDINGS.test.csv\"; #\n",
        "    inputfiledir = inputfiledir + \"Test-InputFiles/\"\n",
        "    #file = \"GR18Plus\"\n",
        "    file = \"EAST5+.test\"\n",
        "    #file = \"test\"\n",
        "\n",
        "else:\n",
        "    feedbacklinecount = 50000\n",
        "    eastcounts = path + \"KNOWN-EAST-HOLDINGS.csv\" ; #\n",
        "    wccounts = path + \"KNOWN-WC-HOLDINGS.csv\"; #\n",
        "    htcounts = path + \"KNOWN-HATHI-HOLDINGS.csv\"; #\n",
        "    inputfiledir = inputfiledir + \"InputFiles/\"\n",
        "    file = \"EAST5+\"\n",
        "\n",
        "############# UPDATE THESE FOR FILE YOU ARE RUNNING #########################\n",
        "libmetadatalibrarycolumn = 1 # 1 is all EAST in GR names, 2 is 2023 CA GR report names. NOTE: PASCAL EAST are herein mapped to EAST23 - use value of 2 for pascal\n",
        "iscombinedfile = False # this used around line 220 - splits control number 001 if is a combined holding file, e.g. PASCAL\n",
        "combinecirc = False # does the file have circ data and do we want to combine it\n",
        "titlerow = 0\n",
        "titlecolumnSubfieldA = 0\n",
        "titlecolumnSubfieldB = 1\n",
        "languagecolumn = 19\n",
        "publishercolumn = 8\n",
        "OCN035column = 22\n",
        "matchoncolumn = 20 # should be ocn (3) or matchkey (20), depending on dataset\n",
        "#########################\n",
        "\n",
        "nyufile = inputfiledir + \"NYU.csv\"\n",
        "input_file = inputfiledir + file + \".\" + extension\n",
        "print(input_file)\n",
        "print(nyufile)\n",
        "output_file = inputfiledir + file + '_modified.' + extension\n",
        "output_csv_file = inputfiledir + \"ForALLFile_\" + file + '.csv'\n",
        "#ipdb.set_trace()\n",
        "output_4EASTAPI_file = inputfiledir + file + '_OCNs4EASTAPI.tsv'\n",
        "output_4HoldingsAPI_file = inputfiledir + file + '_OCNs4HOLDINGSAPI.tsv'\n",
        "\n",
        "output_noOCN_file = inputfiledir + \"ForALLNoOCNFile-\" + file + '.csv'\n",
        "badrows_file = inputfiledir + file + '_badrows.' + extension\n",
        "noCurrentOCLC_file = inputfiledir + 'NoCurrentOCLC.csv'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwWKAbJSAStb",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Read in Gold Rush file\n",
        "# Read the Excel Gold Rush input file and make a pandas dataframe - df\n",
        "\n",
        "if extension == \"xlsx\":\n",
        "    df = pd.read_excel(input_file, sheet_name=0, engine='openpyxl') # uses first sheet\n",
        "elif extension == \"csv\":\n",
        "    df = pd.read_csv(input_file)\n",
        "else:\n",
        "    df = pd.read_csv(input_file, sep='|', on_bad_lines='warn') # Issues a warning when a bad line is found but continues processing the file, skipping the problematic line.\n",
        "    #print(df.columns, \"\\n\\n\")\n",
        "\n",
        "# remove all the PALNI rows, not counting them in this project. .copy avoids SettingWithCopyWarning,\n",
        "df = df[df['Library'] != \"*PALNI\"].copy() # chatGPT suggests .copy efficient enough not to worry\n",
        "\n",
        "df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "df['MatchKey Count'] = df.groupby('MatchKey')['MatchKey'].transform('count')\n",
        "total_rows = len(df)\n",
        "\n",
        "df['Title'] = '' # this will hold merged 245$a 245$b - done in read each line section\n",
        "\n",
        "# Group by matchkey and count to make new 'MatchKey Count' column\n",
        "df['MatchKey Count'] = df.groupby('MatchKey')['MatchKey'].transform('count').fillna(0).astype(int)\n",
        "\n",
        "df['Title'] = '' # this will hold merged 245$a 245$b - done in read each line section\n",
        "\n",
        "print(\"Loaded \", total_rows, \"from input file: \", input_file)\n",
        "#print(df.iloc[1])\n",
        "print(df.columns)\n",
        "#print(df['MatchKey Count'])\n",
        "#print(df['035 field'])\n",
        "#print(df['MatchKey'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5T6ehxo8y_Rg"
      },
      "outputs": [],
      "source": [
        "#@title Read the EAST Libraries Metadata file\n",
        "# A 0: inst_id,\n",
        "# B 1: Name in Gold Rush Reports EAST\n",
        "# C 2: State,\n",
        "# D 3: Symbol\n",
        "# E 4: Est Collection Size\n",
        "# F 5: # in GR\n",
        "# G 6: Percent Collection\n",
        "# H 7: Retention type\n",
        "# I 8: Census Zone\n",
        "# J 9: Region\n",
        "# K 10: Allocation File Name\n",
        "# L 11: URL (blank)\n",
        "# M 12: FEMA Zone\n",
        "# N 13: Name in Members xlsx\n",
        "# O 14: Cohort (1 , 2)\n",
        "# P 15: OCLC Number Location\n",
        "\n",
        "\n",
        "libmetadata_dict = {} # map name to metadata\n",
        "symboltostate_dict = {} # map symbol to state\n",
        "libmetadata_df = pd.read_excel(libmetadata, sheet_name='Plus5', dtype=str) #\n",
        "for index, row in libmetadata_df.iterrows():\n",
        "    #libname: 0:state, 1:symbols, 2:collection size, 3:retentions, 4: %, 5:zone, 6:region, 7:filename, 8:FEMA, 9:Cohort, 10:OCLC Loc\n",
        "    libmetadata_dict[row.iloc[libmetadatalibrarycolumn]] = [ row['State'],row['Symbol'], row['Est Collection Size'],row['# in Gold Rush (2022)'], \\\n",
        "                                                             row['Percent Collection'], row['Census Zone'], row['Region'], row['Allocation File Name'], \\\n",
        "                                                             row['FEMA Zone'], row['Cohort'], row['OCLC Number Location']]\n",
        "    # the following format, using index as position, is depricated - updated above to use iloc or column name\n",
        "    #libmetadata_dict[row[libmetadatalibrarycolumn]] = [ row[2],row[3], row[4],row[5], row[6], row[8], row[9] ]\n",
        "\n",
        "    symbols = row['Symbol'].split() # split symbols on space\n",
        "    for sym in symbols:\n",
        "        symboltostate_dict[sym] = row['State'] # symbol mapped to state\n",
        "\n",
        "    if pd.isna(libmetadata_dict[row.iloc[libmetadatalibrarycolumn]][10]):\n",
        "      print(row.iloc[libmetadatalibrarycolumn], \" Needs OCN Location in libmetadata sheet\")\n",
        "    #print(row.iloc[libmetadatalibrarycolumn], \"=>\", libmetadata_dict[row.iloc[libmetadatalibrarycolumn]][10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uvRS86_5hfNt"
      },
      "outputs": [],
      "source": [
        "#@title Read in EAST Counts df, load to east_dict\n",
        "# Read the TSV eastcounts file into a pandas DataFrame, make a hash on ocn of the east count values\n",
        "# Headers: oclcNumber\tcurrentOCLC\ttotal_retained_count\tretained_symbols\tstatus\n",
        "eastdf = pd.read_csv(eastcounts, dtype=str)\n",
        "east_dict = {}\n",
        "\n",
        "#for index, row in eastdf.iterrows():\n",
        "for row in eastdf.itertuples(index=True):\n",
        "\n",
        "    # make sure I haven't screwed up holdings vs retained api counts\n",
        "    if  bool(re.match(r'^\\d.*', str(row.retained_symbols))): # check and see if symbols starts with digit, which is error\n",
        "        #print(\"Bad row at line\", index+2, \"in KNOWN-EAST-HOLDINGS - maybe WC counts got in here:\", str(row['retained_symbols']))\n",
        "        print(\"Bad row at line\", row.Index + 2, \"in KNOWN-EAST-HOLDINGS - maybe WC counts got in here:\", str(row.retained_symbols))\n",
        "\n",
        "        continue\n",
        "\n",
        "    if row.retained_symbols == 'retained_symbols': # it's a header row, skip\n",
        "        continue\n",
        "    retainedinstate = []\n",
        "    eastretained = float(row.total_retained_count)   # nan values bif -> could float or df['my_column'] = df['my_column'].fillna(0).astype(int)\n",
        "    #              the above might bite you later with column east retained being \"#.0\"\n",
        "    currentOCN = row.currentOCLC\n",
        "\n",
        "    libs = str(row.retained_symbols).split(',')\n",
        "\n",
        "    for sym in libs: # make retained in state and retained by symbol\n",
        "        sym = sym.strip()\n",
        "        if sym != 'nan': # there is a retention\n",
        "            try:\n",
        "                retainedinstate.append(symboltostate_dict[sym])\n",
        "            except Exception as e: # the symbol is not an EAST symbol, report and decrement number of retained copies\n",
        "                print(\"Symbol not in libmetadata:\", sym, \"at row \", index+2, libs)\n",
        "                #print(e)\n",
        "                eastretained -= 1\n",
        "\n",
        "    retainedinstate.sort()\n",
        "    east_dict[row.oclcNumber] = [eastretained, \",\".join(retainedinstate), libs, currentOCN] # OCLC Number:  0:Retentions , 1:States, 2:symbols, 3:current OCN\n",
        "    if (index + 1) % 50000 == 0:\n",
        "        print(f\"Loaded {index + 1} rows into the EAST hash\")\n",
        "        #print(row[0], \":\", east_dict[row[0]])\n",
        "#print(east_dict[row.iloc[0]][0])\n",
        "#print(east_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create df of >5 and has OCN\n",
        "\n",
        "print(\"Known EAST count:\", len(eastdf))\n",
        "# Ensure column 3 is numeric, coercing errors to NaN\n",
        "eastdf.iloc[:, 2] = pd.to_numeric(eastdf.iloc[:, 2], errors='coerce')\n",
        "\n",
        "# Now count rows where col 2 is blank and col 3 > 5\n",
        "count = eastdf[\n",
        "    (eastdf.iloc[:, 1].isna() | (eastdf.iloc[:, 1] == '')) &\n",
        "    (eastdf.iloc[:, 2] > 5)\n",
        "].shape[0]\n",
        "\n",
        "print(\"Rows with EAST Greater Than 5, but no Current OCN - should be 0:\", count)\n"
      ],
      "metadata": {
        "id": "ZnOdtCUcGwjj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YN0TBzyt9BTk"
      },
      "outputs": [],
      "source": [
        "#@title Read in wccounts df, load to wc_dict\n",
        "# Read the TSV wccounts file into a pandas DataFrame, make a hash on ocn of the east count values\n",
        "# columns: oclcNumber\tcurrentOclcNumber\tmergedOCNs\tUS Holdings\tHolding_count\tholding_symbols\ttitle\tstatus\n",
        "wcdf = pd.read_csv(wccounts, dtype=str)\n",
        "wc_dict = {}\n",
        "for index, row in wcdf.iterrows(): # 0:oclcNumber\t1:currentOclcNumber\t2:mergedOCNs\t3:US Holdings\t4:Holding_count\t5:holding_symbols\t6:title\t7:status\n",
        "    if str(row['Holding_count']) == \"success\":  #if  bool(re.search(r'[a-zA-z]', str(row[3]))): # make sure it is a digit, if not print warning and continue\n",
        "        print(\"Bad row at line\", index+2, \"in KNOWN-WC-HOLDINGS - maybe EAST counts got in here, cell 4 is '\" , row['Holding_count'], \"' but should be numeric\")\n",
        "        continue\n",
        "    elif row['status'] == \"failed - invalid OCLC number\":\n",
        "        wc_dict[row['oclcNumber']] = \"Invalid OCLC Number\"\n",
        "    elif row['status'] == \"failed\":\n",
        "        wc_dict[row['oclcNumber']] = \"failed lookup\"\n",
        "    else:\n",
        "        wc_dict[row['oclcNumber']] = row['Holding_count']\n",
        "\n",
        "    if (index + 1) % 50000 == 0:\n",
        "        print(f\"Loaded {index + 1} rows into the WC hash\")\n",
        "        #print(row[0], \":\", east_dict[row[0]])\n",
        "#print(wc_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkgJjxM3_CfF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Read in ht_dict\n",
        "#htdf = pd.read_csv(htcounts, dtype=str, sep='\\t') # not sure why I was doing the df step,(excel?) now just reading from file\n",
        "ht_dict = {}\n",
        "with open(htcounts, 'r') as ht:\n",
        "    reader = csv.reader(ht, delimiter=\"\\t\")\n",
        "    ht_dict = {row[0]: row[1] for row in reader}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Read in NYU_dict\n",
        "#htdf = pd.read_csv(nyufile, dtype=str) #  now just reading from file, though df is fast\n",
        "NYU_dict = {}\n",
        "with open(nyufile, 'r') as nyu:\n",
        "    reader = csv.reader(nyu)\n",
        "    NYU_dict = {row[0]: row[1] for row in reader} # BIBID/001 is key, 079 is value\n",
        "    #NYU_dict = {row[0].lstrip('0'): row[1] for row in reader}  # Strip leading zeros from key (bibid), value is 079\n",
        "    # above was problematic as BIBID/001 not always numberic for other schools so didn't want to set as int\n",
        "print(len(NYU_dict))\n",
        "print(NYU_dict['001187186'])"
      ],
      "metadata": {
        "id": "dERpLUoERCGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9nvF3EOK_TOL"
      },
      "outputs": [],
      "source": [
        "#@title If 'iscombinedfile', split out the library name - dependent on first read of GR file!\n",
        "# Split the data in the 2nd position and update the 3rd position -- ONLY NEED THIS IF COMBINED FILE, EG ODU-1234567\n",
        "if iscombinedfile:\n",
        "    #print(df.iloc[:, 3])\n",
        "    split_data = df.iloc[:, 3].str.split('-', expand=True) # split column D on dash\n",
        "    df.iloc[:, 4] = split_data[0] # library name\n",
        "    df.iloc[:, 3] = split_data[1] # control number\n",
        "    #print(split_data[0])\n",
        "    #print(split_data[1])\n",
        "\n",
        "    #df['Library'] = df['Library'].replace(pascalname_dict) # revive if you want to map PALNI names more?\n",
        "#print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2K6plVESDtLO"
      },
      "outputs": [],
      "source": [
        "#@title Fix up call number / add columns and arrays for east, wc, ht, states, symbols, zone, region, suspect, currentOCN\n",
        "\n",
        "# If callnumber doesn't start with alpha character, make it blank.\n",
        "df['LC Call Number'] = df['LC Call Number'].apply(lambda x: x if re.match(r'^[a-zA-Z]', str(x)) else '')\n",
        "\n",
        "# Make 'LC call number' be the value of the leading alpha numeric characters\n",
        "df['LC Call Number'] = df['LC Call Number'].replace(r'^([a-zA-Z]+[0-9]+).*$', r'\\1', regex=True)\n",
        "\n",
        "# Make a new column, \"LC\" that is just the beginning alpha\n",
        "df['LC'] = df['LC Call Number'].replace(r'^([a-zA-Z]+).*$', r'\\1', regex=True)\n",
        "\n",
        "# if this is the circ sheet, move the circ column to the end so other columns in same order as other output sheets\n",
        "# this might not need to be done anymore...?\n",
        "\n",
        "if \"Circ. Count\" in df.columns:\n",
        "    df[\"Circ. Count\"] = df.pop(\"Circ. Count\")\n",
        "\n",
        "df.loc[:, \"Suspect\"] = \"\" # create column \"Suspect\"\n",
        "df.loc[:, \"Held In States\"] = \"\" # create column \"Held In States\"\n",
        "df.loc[:, \"Held By Symbols\"] = \"\" # create column \"Held In States\"\n",
        "df.loc[:, \"Retained In States\"] = \"\" # create column \"States Retained In\"\n",
        "df.loc[:, \"Retained By Symbol\"] = \"\" # create column \"Retained By Symbol\"\n",
        "df.loc[:, \"Zone\"] = \"\" # create column \"Zone\"\n",
        "df.loc[:, \"Region\"] = \"\" # create column \"Region\"\n",
        "df.loc[:, \"Current OCN\"] = \"\" # create current OCN column\n",
        "\n",
        "\n",
        "# New dataframe which will contain rows without OCN\n",
        "no_ocn_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "# Convert 035 field to text format, rm =, for things like \"=035 ...\" which were being interpretted as broken formulas.\n",
        "# GR might not export '=035' prefix anymore .. so this might no longer be necessary\n",
        "df['035 field'] = df['035 field'].astype(\"string\")\n",
        "df.loc[:, '035 field'] = df.loc[:, '035 field'].str.replace(r'=', '') # works with regex\n",
        "\n",
        "ocnmatch = parse_035()  # defined in Parse035FieldRegEx.py # print(ocnmatch.pattern)\n",
        "titlepattern = title() # defined in MarkSuspectRegEx.py\n",
        "publisherpattern = publisher() # defined in MarkSuspectRegEx.py\n",
        "\n",
        "ocn_values = []\n",
        "rows_to_remove = []\n",
        "bad_rows = []\n",
        "east_counts = [] # array to hold EAST counts\n",
        "east_states = [] # array to hold states in which OCN retained\n",
        "east_symbols = []# array to hold symbols by whom OCN retained\n",
        "wc_counts = []\n",
        "ht_counts = []\n",
        "needs_east_count = []\n",
        "needs_wc_count = []\n",
        "zone = [] # array to hold census zone North South\n",
        "region = [] # array to hold census region\n",
        "suspect = [] # array to hold if title/publisher is on suspect list\n",
        "current_OCN = [] # array to hold current OCN\n",
        "\n",
        "cumulative_circ = {} # also do cum circ here as we are iterating the file\n",
        "cumulative_states = {} # also do states here as we are iterating the file\n",
        "cumulative_holdings_symbols = {} # dict for held by symbols\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Create Title column from 245$a + 245$b\n",
        "# Replace NaN values with empty strings in the title columns\n",
        "df['Title(245$a)'] = df['Title(245$a)'].fillna('')\n",
        "df['Title(245$b)'] = df['Title(245$b)'].fillna('')\n",
        "# Concatenate the title subfields and assign to the 'Title' column\n",
        "df['Title'] = df.apply(lambda row: (row['Title(245$a)'] + ' ' + row['Title(245$b)']).strip(), axis=1)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bvOre99Yy_uq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Fix any bad NYU lines that lack an 035\n",
        "# should also maybe here just update all 035s with 079?? NYU_dict\n",
        "#for i in range(len(df)):\n",
        " # ipdb.set_trace()\n",
        "\n",
        "#mask = (df['035 field'].isna() | (df['035 field'] == '')) & (df['Library'] == 'New York University')\n",
        "mask = ( df['035 field'].isna() | (df['035 field'] == '') | (df['035 field'] == '<NA>') ) & (df['Library'] == 'New York University')\n",
        "#print(df[mask])\n",
        "\n",
        "\n",
        "print(\"Bad NYU Lines that will try to fix: \", len(df[mask]), \" these are no 035, bad 035s handled elsewhere\")\n",
        "for idx, row in df[mask].iterrows():\n",
        "    key = row['Control Number(001)']\n",
        "    #ipdb.set_trace()\n",
        "    if key in NYU_dict:\n",
        "        df.at[idx, '035 field'] = NYU_dict[key]"
      ],
      "metadata": {
        "id": "h71kRBD0P1K8",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIWoQAP6FFW5"
      },
      "outputs": [],
      "source": [
        "#@title Iterate DF, find OCNs, do EAST etc. Lookups, Create Output Arrays\n",
        "badrowscount = 0\n",
        "\n",
        "for i in range(len(df)): # this section works on making new OCLC column, and makes dictionary of cumulative circ and states\n",
        "\n",
        "    ocns = [] # reset each run\n",
        "    matchkey = df.loc[i,'MatchKey'] # use matchkey as key for states and cum circ and holding symbols\n",
        "\n",
        "    if combinecirc: # only do this if we have circ in this file\n",
        "      if \"Circ. Count\" in df.columns: # if we have a file with circ, make dict to count total circ per matchkey\n",
        "          if pd.isna(df.loc[i,'Circ. Count']):  ## NEED TO CHECK HERE THAT NOT BLANK OR UNDEF CIRC\n",
        "            df.loc[i,'Circ. Count'] = 0 # assign 0 to any blanks - this might be a bad idea? is there a difference between zero and no info?\n",
        "          if matchkey in cumulative_circ.keys():\n",
        "              cumulative_circ[matchkey] = cumulative_circ[matchkey] + df.loc[i,'Circ. Count']\n",
        "          else:\n",
        "              cumulative_circ[matchkey] = df.loc[i,'Circ. Count']\n",
        "\n",
        "    try: # check to make sure input file has valid value in 'library' column\n",
        "      libmetadata_dict[df.loc[i, 'Library']][10]\n",
        "    except Exception as e: #\n",
        "      #print(e , \" at \", i)\n",
        "      #print(df.loc[i])\n",
        "      # these usually are blank rows.... note NYU blank 035s fixed above\n",
        "      badrowscount = badrowscount+1\n",
        "      bad_rows.append(i)\n",
        "\n",
        "      ocn_values.append('')\n",
        "      east_counts.append('')\n",
        "      east_states.append('')\n",
        "      east_symbols.append('')\n",
        "      wc_counts.append('')\n",
        "      ht_counts.append('')\n",
        "      region.append('')\n",
        "      zone.append('')\n",
        "      suspect.append('')\n",
        "      current_OCN.append('')\n",
        "      continue\n",
        "      #print(df.loc[i])\n",
        "      #ipdb.set_trace()\n",
        "\n",
        "    if libmetadata_dict[df.loc[i, 'Library']][10] == \"035 field\": # if key error here something wrong in source data from GR\n",
        "      try: # Split the values in column 20 based on \"\\s\" and loop over the array to find OCN\n",
        "\n",
        "        temp = df.loc[i, '035 field']\n",
        "        temp = re.sub(r'\\s+\\$9', '$9', temp) # Keeping $9ExL with ocns for checking for SCSU -  $a(scbph)b10566740-01pascal_scsu $a00846736 $9ExL\n",
        "        temp = re.sub(r'LC\\)\\s+', 'LC)', temp) # first remove spaces that seem legit  $a(OCoLC) 1154664360, then,\n",
        "        temp = re.sub(r'\\s+$', '', temp) # remove trailing whitespace\n",
        "        ocns = re.split('\\s+', temp) # split 035 on whitespace, a little wacky but seems to work\n",
        "        #ipdb.set_trace()\n",
        "      except Exception as e: # these are blank 035s usually\n",
        "        #print(\"error: \", e)\n",
        "        #print(df.iloc[i,20])\n",
        "        #print(ocns)\n",
        "        ocn_values.append('')\n",
        "        east_counts.append('')\n",
        "        east_states.append('')\n",
        "        east_symbols.append('')\n",
        "        wc_counts.append('')\n",
        "        ht_counts.append('')\n",
        "        region.append('')\n",
        "        zone.append('')\n",
        "        suspect.append('')\n",
        "        current_OCN.append('')\n",
        "\n",
        "        no_ocn_df[len(no_ocn_df)] = df.iloc[i, :]\n",
        "        rows_to_remove.append(i)\n",
        "        continue\n",
        "\n",
        "    else: # assuming ocn in 001, also assuming not blank\n",
        "      #ipdb.set_trace()\n",
        "      ocn = df.loc[i, 'Control Number(001)']\n",
        "      # Convert ocn to string if it's not already - might be blank?\n",
        "      ocn = str(ocn)\n",
        "      #try:\n",
        "      ocn = re.sub(r'^[a-z]+', '', ocn)  #remove leading letters\n",
        "      ocn = re.sub(r'\\s+$','', ocn) # remove trailing whitespace\n",
        "      ocn = re.sub(r'^[0]+', '', ocn)  #remove leading zeros\n",
        "      ocn = \"(OCoLC)\" + ocn # add a prefix so we know this is ocn in the next section\n",
        "      #ocn = ocn.str.replace(r'^[a-z]+', '', regex=True) #remove leading letters\n",
        "      #ocn = ocn.str.replace(r'^[0]+', '', regex=True) #remove leading zeros\n",
        "      ocns = [ocn] # put the ocn in an ocns array\n",
        "      # except Exception as e:\n",
        "      #  print(e, \"001 was:\", ocn)\n",
        "      #  print(df.loc[i])\n",
        "\n",
        "      # might be more effient to do a lot of this by slices - e.g. make oclcNumber column\n",
        "\n",
        "    for oclcnumb in ocns:  #\n",
        "        if (len(ocns) < 1):\n",
        "          ipdb.set_trace() # leaving this in for now as it really shouldn't happen and want to see it if it does\n",
        "        oclcnumb = re.sub(r'^.*?\\$a', \"\", oclcnumb) # remove anything up to and including first $a (gets the 035 field prefix that Gold Rush puts in)\n",
        "        oclcnumb = re.sub(r'\\\\+$', \"\", oclcnumb) # remove any trailing slashes (weird that there are some...)\n",
        "        oclcnumb = re.sub(r'\\(CStRLIN\\)', '', oclcnumb) #  $a(CStRLIN)ocn476563480$9ExL;\n",
        "        oclcnumb = re.sub(r'\\(DLC.*\\)', '', oclcnumb) # $a(DLC    )ocn478351245$9ExL;\n",
        "        oclcnumb = re.sub(r'\\(\\(OCoLC\\)\\)', '(OCoLC)', oclcnumb) # Bowdoin had some ((OCoLC))\n",
        "        #ipdb.set_trace()\n",
        "\n",
        "        ### somtimes these already start with OCoLC\n",
        "        if str(df.loc[i, 'Library']).startswith(\"South Carolina State University\") and re.search('\\$9ExL', oclcnumb) and not oclcnumb.startswith(\"(OCoLC)\"): # hack for SCSU no prefix but is OCN $9ExL\n",
        "            oclcnumb = \"(OCoLC)\" + oclcnumb\n",
        "        if str(df.loc[i, 'Library']).startswith(\"Bryn Mawr\") and re.search('\\$9ExL', oclcnumb) and not oclcnumb.startswith(\"(OCoLC)\"): # hack for Brwn Mawr $a(TriCo)b16376626-01tri_inst $a9557018 $9ExL\n",
        "            oclcnumb = \"(OCoLC)\" + oclcnumb\n",
        "        if str(df.loc[i, 'Library']).startswith(\"Adelphi\") and re.search('\\$9ExL', oclcnumb) and not oclcnumb.startswith(\"(OCoLC)\"): # hack for Adelphi $a(NGcA)b13378910-01adelphi_inst $a20825739 $9ExL\n",
        "            oclcnumb = \"(OCoLC)\" + oclcnumb\n",
        "        if str(df.loc[i, 'Library']).startswith(\"Swarthmore\") and re.search('\\$9ExL', oclcnumb) and not oclcnumb.startswith(\"(OCoLC)\"): # hack for Adelphi $a(NGcA)b13378910-01adelphi_inst $a20825739 $9ExL\n",
        "            oclcnumb = \"(OCoLC)\" + oclcnumb\n",
        "\n",
        "        if '$' in oclcnumb: # if has $ -e.g. a $z or $9 - remove all of that\n",
        "            oclcnumb = oclcnumb.split('$')[0]\n",
        "\n",
        "        # NYU had some OCNs in 079 |a(OCoLC)244475171 get it and make it the oclc number. blank 035s already dealt with above\n",
        "        if not ocnmatch.search(oclcnumb) and df.loc[i, 'Library'] == \"New York University\":\n",
        "          key = df.loc[i, 'Control Number(001)']\n",
        "          if key in NYU_dict:\n",
        "            if test: #002114878,(OCoLC)28114500\n",
        "              print(\"bibid\", key, \"OCN:\", NYU_dict[key], \"at\", i, \"ocn was\", oclcnumb, \"this should print just one bibid\" ) # in test data this should only print once, no 035 should have been fixed by here.\n",
        "              assert(key == \"2114878\")\n",
        "            df.loc[i, '035 field'] = NYU_dict[key] # don't really need to do this - too late by here.\n",
        "            oclcnumb = NYU_dict[key]\n",
        "\n",
        "        if ocnmatch.search(oclcnumb) or \\\n",
        "          (str(df.loc[i, 'Library']).startswith(\"Lafayette\") and oclcnumb.isnumeric()) or \\\n",
        "          (str(df.loc[i, 'Library']).startswith(\"ODU\") and oclcnumb.isnumeric()) or \\\n",
        "          (str(df.loc[i, 'Library']).startswith(\"Bryn Mawr\") and oclcnumb.isnumeric()):   # # 'or' hack ODU-001 ^ODU && no prefix is an ocn, Lafayette, there were ~350 Bryn Mawr of these - might be problematic\n",
        "\n",
        "            oclcnumb = re.sub(ocnmatch, \"\", oclcnumb) # remove the matched prefix\n",
        "            oclcnumb = re.sub(\"^0+\", \"\", oclcnumb) # remove leading zeros\n",
        "            #print(oclcnumb)\n",
        "            if oclcnumb.isnumeric() or oclcnumb.isdigit(): # oclcnumb might be an int or a sting at this point...might not need this or\n",
        "                ocn_values.append(oclcnumb)\n",
        "                libraryname = df.loc[i, 'Library']   # doing state count here so only counts states in records that have an OCN\n",
        "                #ipdb.set_trace()\n",
        "                #libmetadatalibrarycolumn = [ row['State'],row['Symbol'], row['Est Collection Size'],row['# in Gold Rush (2022)'], row['Percent Collection'], row['Census Zone'], row['Region'] ]\n",
        "                zone.append(libmetadata_dict[libraryname][5]) # #libname: 0:state, 1:symbols, 2:collection size, 3:retentions, 4: percent, 5:census zone, 6:region\n",
        "                region.append(libmetadata_dict[libraryname][6]) # #libname: 0:state, 1:symbols, 2:collection size, 3:retentions, 4: percent, 5:census zone, 6:region\n",
        "\n",
        "                if matchkey in cumulative_states.keys():                 # loading up the states dictionary, appending if matchkey already seen\n",
        "                    cumulative_states[matchkey] = cumulative_states[matchkey] + ', ' + str(libmetadata_dict[libraryname][0])\n",
        "                else:\n",
        "                    cumulative_states[matchkey] = libmetadata_dict[libraryname][0]\n",
        "\n",
        "                holdings_symbol = libmetadata_dict[libraryname][1].split() # want just the first symbol if there are multi\n",
        "                if matchkey in cumulative_holdings_symbols.keys():                 # loading up the holdings symbols dictionary, appending if matchkey already seen\n",
        "                    cumulative_holdings_symbols[matchkey] = cumulative_holdings_symbols[matchkey] + ', ' + str(holdings_symbol[0])\n",
        "                else:\n",
        "                    cumulative_holdings_symbols[matchkey] = holdings_symbol[0]\n",
        "\n",
        "                if oclcnumb in east_dict: # do we have an east count for this, and retained states\n",
        "                    # east_dict => OCLC Number:  0:Retentions , 1:States, 2:symbols, 3:current OCN\n",
        "                    east_counts.append(east_dict[oclcnumb][0])\n",
        "                    east_states.append(east_dict[oclcnumb][1])\n",
        "                    east_symbols.append(\",\".join(east_dict[oclcnumb][2]))\n",
        "                    current_OCN.append(east_dict[oclcnumb][3])\n",
        "\n",
        "                else:\n",
        "                    east_counts.append('')\n",
        "                    east_states.append('')\n",
        "                    east_symbols.append('')\n",
        "                    current_OCN.append('')\n",
        "                    needs_east_count.append(oclcnumb)\n",
        "                if oclcnumb in wc_dict: # do we have a wc count for this\n",
        "                    wc_counts.append(wc_dict[oclcnumb])\n",
        "                else:\n",
        "                    wc_counts.append('')\n",
        "                    needs_wc_count.append(oclcnumb)\n",
        "                if oclcnumb in ht_dict: # do we have a ht rights for this\n",
        "                    ht_counts.append(ht_dict[oclcnumb])\n",
        "                else:\n",
        "                    ht_counts.append('')\n",
        "\n",
        "                # # Finding and marking suspect titles (doing here since known ocn)\n",
        "\n",
        "                title = str(df.loc[i, \"Title\"])\n",
        "                language  = str(df.iloc[i,languagecolumn])\n",
        "                publisher = str(df.iloc[i,publishercolumn]) # weirdly there were some int publishers...\n",
        "                #print(titlepattern.search(title))\n",
        "                if title is not None and language == \"English\" and titlepattern.search(title):\n",
        "                    suspect.append(\"title\")\n",
        "                elif publisher is not None and publisherpattern.search(publisher):\n",
        "                    suspect.append(\"publisher\")\n",
        "                else:\n",
        "                    suspect.append('') # empty if doesn't match title or publisher (this cleans up previous runs and regex fixes)\n",
        "                break\n",
        "    else: # else we went through all the v in value w/o finding an ocn # This else runs if the for loop didn't break\n",
        "      ocn_values.append('')\n",
        "      east_counts.append('')\n",
        "      east_states.append('')\n",
        "      east_symbols.append('')\n",
        "      wc_counts.append('')\n",
        "      ht_counts.append('')\n",
        "      region.append('')\n",
        "      zone.append('')\n",
        "      suspect.append('')\n",
        "      current_OCN.append('')\n",
        "\n",
        "      #no_ocn_df = no_ocn_df.append(df.iloc[i, :], ignore_index=True)  # Add row to no OCN dataframe\n",
        "      no_ocn_df[len(no_ocn_df)] = df.iloc[i, :]\n",
        "      rows_to_remove.append(i)\n",
        "      #ipdb.set_trace()\n",
        "\n",
        "print(\"Bad Rows: \", badrowscount)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ucHA8xF3YAP8"
      },
      "outputs": [],
      "source": [
        "#@title Create Output frames and files\n",
        "\n",
        "# Add the OCN and EAST values as a new columns\n",
        "print(\"length ocn_values    : \", len(ocn_values))\n",
        "print(\"length df            : \", len(df))\n",
        "print(\"length wc            : \", len(wc_counts))\n",
        "print(\"length ht            : \", len(ht_counts))\n",
        "print(\"length east_symbols  : \", len(east_symbols))\n",
        "print(\"length region        : \", len(region))\n",
        "print(\"length zone          : \", len(zone))\n",
        "print(\"length suspect       : \", len(suspect))\n",
        "print(\"lenght current ocn.  : \", len(current_OCN))\n",
        "\n",
        "df['OCN'] = ocn_values\n",
        "df['EAST Counts'] = east_counts\n",
        "df['States Retained In'] = east_states\n",
        "df['Retained By Symbol'] = east_symbols\n",
        "df['WorldCat Holdings'] = wc_counts\n",
        "df['HathiTrust'] = ht_counts\n",
        "df['Region'] = region\n",
        "df['Zone'] = zone\n",
        "df['Suspect'] = suspect\n",
        "df['Current OCN'] = current_OCN\n",
        "\n",
        "# Create a new DataFrame with the dropped rows\n",
        "dropped_df = df.loc[rows_to_remove]\n",
        "dropped_df['From Sheet'] = \"NO_OCN\"\n",
        "# Drop the rows that need to be removed from the original DataFrame - no ocns\n",
        "df = df.drop(rows_to_remove)\n",
        "\n",
        "# Create a new DataFrame with the bad data rows\n",
        "bad_df = df.loc[bad_rows] # this bad_df gets written out at the end\n",
        "# Drop the rows that need to be removed from the original DataFrame - bad rows\n",
        "df = df.drop(bad_rows)\n",
        "\n",
        "# add from sheet to df\n",
        "match file:\n",
        "    case \"Unique_0000_1985\":\n",
        "        df['From Sheet'] = 'Unique'\n",
        "    case \"Unique_1986_1999\":\n",
        "        df['From Sheet'] = 'Unique'\n",
        "    case \"Unique_2000_2010\":\n",
        "        df['From Sheet'] = 'Unique'\n",
        "    case \"Overlap_3_4\":\n",
        "        df['From Sheet'] = 'Topping Up'\n",
        "    case \"HighCircLowEAST_MoreThan10\":\n",
        "        df['From Sheet'] = 'High Circ Low EAST'\n",
        "    case \"Pre1900_BOOK_2_5\":\n",
        "        df['From Sheet'] = 'Pre1901'\n",
        "    case _:\n",
        "         df['From Sheet'] = file\n",
        "\n",
        "# Remove specified columns\n",
        "#columns_to_remove = [4, 6, 8, 10, 11, 12, 13, 15, 16]\n",
        "#df = df.drop(df.columns[columns_to_remove], axis=1)\n",
        "#ipdb.set_trace()\n",
        "df.drop(columns=['Branch', 'ISSN', 'Publisher Number', 'Dewey Call Number', 'SuDocs Call Number', 'Report Number', 'General Report Number', 'Action Note(583$f)', 'Action Note(583$h)'], inplace=True)\n",
        "# reorder columns from stack overflow : In the end, the point is: df = df[ list with newly arranged column names ] e.g. df = df[['mean', 'average', '1', '2', '3']]\n",
        "#ipdb.set_trace()\n",
        "df.rename(columns = {'Gov Docs(086$a)':'Doc: 086', 'LC':'LC Class', 'LC Call Number':'LC'}, inplace = True)\n",
        "dropped_df.rename(columns = {'Gov Docs(086$a)':'Doc: 086', 'LC':'LC Class', 'LC Call Number':'LC'}, inplace = True)\n",
        "\n",
        "#df = df[['Title', 'Author', 'Library', 'Control Number(001)', 'ISBN', 'Publisher Name', 'LC', 'LC Class','Edition', 'Publication Date', 'Language', 'MatchKey', '035 field', 'OCN', 'EAST Counts', 'WorldCat Holdings', 'From Sheet', 'Circ. Count', 'Juvenile Lit.', 'HathiTrust', 'Doc: 086', 'Suspect', 'States Retained In','Held In States', 'Zone', 'Retained By Symbol', 'Held By Symbols', 'Region']]\n",
        "df = df[['Title', 'Author', 'Library', 'Control Number(001)', 'ISBN', 'Publisher Name', 'LC', 'LC Class','Edition', 'Publication Date', 'Language', 'MatchKey', '035 field', 'OCN', 'Current OCN', 'EAST Counts', 'MatchKey Count', 'WorldCat Holdings', 'Juvenile Lit.', 'HathiTrust', 'Doc: 086', 'Suspect', 'States Retained In','Held In States', 'Zone', 'Retained By Symbol', 'Held By Symbols', 'Region']]\n",
        "                                                                                                                                                                                         #print(df['MatchKey Count'])\n",
        "dropped_df = dropped_df[['Title', 'Author', 'Library', 'Control Number(001)', 'ISBN', 'Publisher Name', 'LC', 'LC Class','Edition', 'Publication Date', 'Language', 'MatchKey', '035 field', 'OCN', 'Current OCN', 'EAST Counts', 'WorldCat Holdings', 'From Sheet']]\n",
        "\n",
        "# using set() to remove duplicates from list of OCNs 4 API\n",
        "needs_east_count = list(set(needs_east_count))\n",
        "needs_wc_count = list(set(needs_wc_count))\n",
        "\n",
        "if combinecirc: # if this file has circ and we combined it\n",
        "  if \"Circ. Count\" in df.columns: ## fix up circ counts to be cumulative based on matchkey built in cum circ dict\n",
        "    #df[\"Circ. Count\"] = df[\"MatchKey\"].map(cumulative_circ) #\n",
        "    df[\"Circ. Count\"] = df.iloc[:, matchoncolumn].map(cumulative_circ) # note using matchoncolumn varialble not 'MatchKey' column\n",
        "\n",
        "#ipdb.set_trace()\n",
        "#df[\"Held In States\"] = df[\"MatchKey\"].map(cumulative_states) # add states\n",
        "#df[\"Held By Symbols\"] = df[\"MatchKey\"].map(cumulative_holdings_symbols) # add held by symbols\n",
        "\n",
        "df[\"Held In States\"] = df.iloc[:, matchoncolumn].map(cumulative_states) # add states\n",
        "df[\"Held By Symbols\"] = df.iloc[:, matchoncolumn].map(cumulative_holdings_symbols) # add held by symbols\n",
        "\n",
        "\n",
        "# Save the modified DataFrame to a new Excel file\n",
        "df['Control Number(001)'] = df['Control Number(001)'].astype(str) # make sure 001 is string - prevents mmsids from being munged\n",
        "\n",
        "if extension == \"xlsx\":\n",
        "    with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer: # create a excel writer object\n",
        "        df.to_excel(writer, sheet_name=\"Sheet1\", index=False, header=True, freeze_panes=(1,0)) #this fails to create valid file if cell started with \"=\" and is not an equation\n",
        "        dropped_df.to_excel(writer, sheet_name=\"no OCN\", index=False)\n",
        "\n",
        "        # create new df for ocns w/o an EAST count that will write to new sheet\n",
        "        pd.DataFrame({\"oclcNumber\": needs_east_count}).to_excel(writer, sheet_name=\"OCLC4EASTAPI\", index=False)\n",
        "        pd.DataFrame({\"oclcNumber\": needs_wc_count}).to_excel(writer, sheet_name=\"OCLC4HoldingsAPI\", index=False)\n",
        "\n",
        "        # Access the XlsxWriter workbook and worksheet objects\n",
        "        workbook  = writer.book\n",
        "        worksheet = writer.sheets[\"Sheet1\"]\n",
        "\n",
        "        #text_format = workbook.add_format({'num_format': '0'}) # 0 works better than @, @ gives long numbers in scientific notation\n",
        "\n",
        "        # Set column - first column, last column, width, format, options (optional)\n",
        "        #worksheet.set_column('A:D', 20, text_format)  # Column A:D width\n",
        "        worksheet.set_column('A:D', 20)  # Column A:D width\n",
        "        worksheet.set_column('E:E', 15)  # Column E width\n",
        "        worksheet.set_column('F:F', 20)  # Column F width\n",
        "        worksheet.set_column('G:G', 5)  # Column G width\n",
        "        worksheet.set_column('J:J', 12)  # Column J width\n",
        "        worksheet.set_column('V:W', 20)  # Column V thru W width\n",
        "\n",
        "\n",
        "        # Define a format for wrapped text\n",
        "        header_format = workbook.add_format({'text_wrap': True, 'bold': True, 'bg_color': 'silver'}) # silver is #C0C0C0\n",
        "        # Apply the format to the header row\n",
        "        for col_num, value in enumerate(df.columns.values):\n",
        "            worksheet.write(0, col_num, value, header_format)\n",
        "else: # else output as csv\n",
        "    df.to_csv(output_file, encoding='utf-8-sig', index=False)     # this will be the same as ForALL file, so a bit redundant\n",
        "\n",
        "pd.DataFrame({\"oclcNumber\": needs_east_count}).to_csv(output_4EASTAPI_file, sep='\\t', index=False)\n",
        "# if test:\n",
        "#   print(needs_east_count)\n",
        "pd.DataFrame({\"oclcNumber\": needs_wc_count}).to_csv(output_4HoldingsAPI_file, sep='\\t', index=False)\n",
        "\n",
        "if combinecirc:\n",
        "  df['Circ. Count'] = df['Circ. Count'].astype(int) # circ is float64 to account for nan, changed to int for export to avoid 0.0\n",
        "  #print(df.dtypes)\n",
        "\n",
        "#df.to_csv(output_tsv_file, sep='\\t', encoding='utf-8', index=False)     # this had encoding issues, and utf-8-sig messed up tsv, now using csv\n",
        "df.to_csv(output_csv_file, encoding='utf-8-sig', index=False)     # Save 'Sheet1' as a CSV file for final merged sheet\n",
        "dropped_df.to_csv(output_noOCN_file, encoding='utf-8-sig', index=False) # Save the no OCN df/sheet to a csv for final merged sheet\n",
        "bad_df.to_csv(badrows_file, encoding='utf-8-sig', index=False)\n",
        "\n",
        "# End the timer and report how long it took\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "minutes = int(elapsed_time // 60)\n",
        "seconds = int(elapsed_time % 60)\n",
        "print(\"----Summary---\")\n",
        "print(total_rows, \" rows were in input file\")\n",
        "print(len(rows_to_remove), \" rows lacked an OCN\")\n",
        "print(len(needs_east_count), \" ONCs Need an EAST count API Lookup\")\n",
        "print(len(needs_wc_count), \" ONCs Need a WC holdings Count API Lookup\")\n",
        "print(\"Bad Rows: \", badrowscount)\n",
        "now = datetime.now()\n",
        "pacific_tz = ZoneInfo('America/Los_Angeles')\n",
        "now_pacific = now.astimezone(pacific_tz)\n",
        "print(f\"\\nEnding at: {now_pacific.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Script took {minutes} minutes and {seconds} seconds to run.\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNZlNXdPU1uaDPdothfF3Q5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}