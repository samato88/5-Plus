{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samato88/5-Plus/blob/main/FilterDataFrame_EAST5Plus_Allocate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More than 5 retentions w/d allocate project"
      ],
      "metadata": {
        "id": "aQcKBxoAmQ9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO DO:\n",
        "Exclude non participants<br>\n",
        "Now at allocate step - can finish this off<br>\n",
        "Make xlsx by lib <br>\n",
        "For comparison make a count of overlap by matchkey<br>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0bSitS88wTSu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "jbv163ibaxwn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cfe22ee-d64d-416e-d7ed-36e7570c7541",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your runtime has 54.8 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ]
        }
      ],
      "source": [
        "#@title Memory and GPU info\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "dDWDWJqq7SsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f3b553f-c117-409d-dcc4-e940b91b0d01",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#@title import google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#drive.flush_and_unmount()  # Unmount existing mount - trying since not reading new files\n",
        "#drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title code to skip cells - %%skip\n",
        "\n",
        "from IPython.core.magic import register_cell_magic\n",
        "\n",
        "@register_cell_magic\n",
        "def skip(line, cell):\n",
        "    return"
      ],
      "metadata": {
        "id": "-qTtOJIWS1EK",
        "cellView": "form"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "vnci7FIqnFen",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f355c0-37d9-49fc-fda4-85d17cf94a79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xlsxwriter in /usr/local/lib/python3.11/dist-packages (3.2.5)\n",
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/Balance_North_South.ipynb to script\n",
            "[NbConvertApp] Writing 5208 bytes to /content/drive/MyDrive/Colab Notebooks/Balance_North_South.py.txt\n",
            "Starting at: 2025-08-08 10:02:20\n"
          ]
        }
      ],
      "source": [
        "#@title imports, set start time\n",
        "!pip install xlsxwriter\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime\n",
        "from zoneinfo import ZoneInfo\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl import load_workbook\n",
        "import csv\n",
        "import numpy as np # Keep this import in the notebook as well\n",
        "from openpyxl.utils.dataframe import dataframe_to_rows\n",
        "\n",
        "# Convert the notebook to a .py file and import a module\n",
        "!jupyter nbconvert --to script /content/drive/MyDrive/Colab\\ Notebooks/Balance_North_South.ipynb --output /content/drive/MyDrive/Colab\\ Notebooks/Balance_North_South.py\n",
        "\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/Colab Notebooks/')\n",
        "# Import from the converted .py file\n",
        "from Balance_North_South import balance_north_south\n",
        "\n",
        "start_time = datetime.now(tz=ZoneInfo('America/Los_Angeles'))\n",
        "print(f\"Starting at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "eblRl2arM9U-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e305a03d-2283-4226-bd4d-86217948e189"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "infile:        /content/drive/MyDrive/Colab Notebooks/5+/InputFiles/ForALLFile_EAST5+.csv\n",
            "outputdir:     /content/drive/MyDrive/Colab Notebooks/5+/Reports/\n",
            "outputallfile: /content/drive/MyDrive/Colab Notebooks/5+/Reports/ALL.csv\n"
          ]
        }
      ],
      "source": [
        "#@title Is this a test run and file locs and global vars (max retain)\n",
        "testing = False\n",
        "cloud = True\n",
        "maxretain = 5\n",
        "\n",
        "if not cloud:\n",
        "    path = \"/Users/samato/Dropbox/EAST/Gold_Rush/Comparisons/Scripts/\"\n",
        "    inputfiledir = \"/Users/samato/Dropbox/EAST/Comparisons/MoreThanFiveEast-2023/\" ;\n",
        "    libmetadata = \"/Users/samato/Dropbox/EAST/Gold_Rush/Comparisons/Scripts/Member_Metadata.xlsx\";  #\n",
        "\n",
        "    if testing:\n",
        "        dir = \"/Users/samato/Dropbox/EAST/Gold_Rush/Comparisons/Scripts/Test/\"\n",
        "        outputdir = dir + \"Reports/\"\n",
        "        infile = dir + \"ForAllFile_test2.csv\"\n",
        "        infilenoocn = dir + \"ForALLNoOCNFile-test.csv\"\n",
        "        feedbackcounter = 5\n",
        "        fields = \"/Users/samato/Dropbox/EAST/Gold_Rush/Post-2010-Group-Collection-Analysis-2023/GR_Reports/Fields.xlsx\"\n",
        "        libmetadata = \"/Users/samato/Dropbox/EAST/Gold_Rush/Comparisons/Scripts/Member_Metadata.xlsx\";  #0:instID, 1:lib, 2:GRLibName, 3:State, 4:Sym, 5:CollSize, 6:Retentions, 7:%, 8:Type, 9:Zone, 10:Region\n",
        "\n",
        "    else:\n",
        "        dir = \"/Users/samato/Dropbox/EAST/Gold_Rush/Post-2010-Group-Collection-Analysis-2023/GR_Reports/AllFiles/\" ;\n",
        "        outputdir = dir + \"Reports/\"\n",
        "        infile = dir + \"ALL.csv\"\n",
        "        infilenoocn = dir + \"ALL.NoOCN.csv\"\n",
        "        feedbackcounter = 50000\n",
        "        fields = \"/Users/samato/Dropbox/EAST/Gold_Rush/Post-2010-Group-Collection-Analysis-2023/GR_Reports/Fields.xlsx\"\n",
        "        libmetadata = \"/Users/samato/Dropbox/EAST/Gold_Rush/Comparisons/Scripts/Member_Metadata.xlsx\";  #0:instID, 1:lib, 2:GRLibName, 3:State, 4:Sym, 5:CollSize, 6:Retentions, 7:%, 8:Type, 9:Zone, 10:Region\n",
        "\n",
        "if cloud:\n",
        "    path = \"/content/drive/MyDrive/Colab Notebooks/5+/\"\n",
        "    libmetadata = \"/content/drive/MyDrive/Colab Notebooks/Includes/Member_Metadata.xlsx\"; # state, % retained\n",
        "    fields = \"/content/drive/MyDrive/Colab Notebooks/Includes/Fields.xlsx\"; # state, % retained\n",
        "\n",
        "    if testing:\n",
        "        #filestub = 'GR18Plus'\n",
        "        filestub = 'EAST5+.test'\n",
        "        filename = 'ForALLFile_' + filestub + '.csv'\n",
        "        feedbackcounter = 2\n",
        "        infile = path + 'Test-InputFiles/' + filename\n",
        "        outputdir = path + 'Test-Reports/'\n",
        "        outputallfile = outputdir + 'ALL.csv'\n",
        "        infilenoocn = path + 'Test/' + 'ForALLNoOCNFile-' + filestub + '.csv'\n",
        "    else:\n",
        "        filestub = 'EAST5+'\n",
        "        filename = 'ForALLFile_' + filestub + '.csv'\n",
        "        feedbackcounter = 50000\n",
        "        infile = path + 'InputFiles/' + filename\n",
        "        outputdir = path + 'Reports/'\n",
        "        outputallfile = outputdir + 'ALL.csv'\n",
        "        infilenoocn = path + 'ForALLNoOCNFile-' + filestub + '.csv' # in this case this is the same as the _modified.txt file\n",
        "\n",
        "print(\"infile:       \", infile)\n",
        "print(\"outputdir:    \", outputdir)\n",
        "print(\"outputallfile:\", outputallfile)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "| LibMetadata                      | Index | Input File PALNI       | Index |\n",
        "|----------------------------------|-------|--------------------------|--------|\n",
        "| Inst_ID                          | 0     | A Title                  | 0      |\n",
        "| Name in Gold Rush Reports EAST  | 1     | B Author                 | 1      |\n",
        "| State                            | 2     | C Library                | 2      |\n",
        "| Symbol                           | 3     | D Control Number(001)    | 3      |\n",
        "| Est Collection Size              | 4     | E ISBN                   | 4      |\n",
        "| # in Gold Rush (2022)           | 5     | F Publisher Name         | 5      |\n",
        "| Percent Collection               | 6     | G LC                     | 6      |\n",
        "| Retention Partners               | 7     | H LC Class               | 7      |\n",
        "| Census Zone                      | 8     | I Edition                | 8      |\n",
        "| Region                           | 9     | J Publication Date       | 9      |\n",
        "| Allocation File Name             | 10    | K Language               | 10     |\n",
        "| URL                              | 11    | L MatchKey               | 11     |\n",
        "| FEMA Zone                        | 12    | M 035 field              | 12     |\n",
        "| Name in Members xlsx             | 13    | N OCN                    | 13     |\n",
        "| Cohort                           | 14    | O EAST Counts            | 14     |\n",
        "| OCLC Number Location             | 15    | P PALNI Counts           | 15     |\n",
        "|                                  |       | Q PALNI Holders          | 16     |\n",
        "|                                  |       | R Indiana Counts         | 17     |\n",
        "|                                  |       | S WorldCat Holdings      | 18     |\n",
        "|                                  |       | T From Sheet             | 19     |\n",
        "|                                  |       | U Juvenile Lit.          | 20     |\n",
        "|                                  |       | V HathiTrust             | 21     |\n",
        "|                                  |       | W Doc: 086               | 22     |\n",
        "|                                  |       | X Suspect                | 23     |\n",
        "|                                  |       | Y States Retained In     | 24     |\n",
        "|                                  |       | Z Held In States         | 25     |\n",
        "|                                  |       | AA Zone                  | 26     |\n",
        "|                                  |       | AB Retained By Symbol    | 27     |\n",
        "|                                  |       | AC Held By Symbols       | 28     |\n",
        "|                                  |       | AD Region                | 29     |\n"
      ],
      "metadata": {
        "id": "uqhDqT-uZvvK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "cellView": "form",
        "id": "7cMrS5MO2oxQ"
      },
      "outputs": [],
      "source": [
        "#@title def updatePercent\n",
        "def updatePercent(libmetadata_dict, librarytoretain):\n",
        "    # libname: 0:state,1:symbols,2:collection size 3:retentions,4:%,5:census zone 6:allocated,7:unique,8:topping,9:suspect,10:noOCN\n",
        "    retentions = libmetadata_dict[librarytoretain][3]\n",
        "    collectionsize = libmetadata_dict[librarytoretain][2]\n",
        "    newpercent = int(retentions) / int(collectionsize)\n",
        "    newpercent = retentions / collectionsize     # calculate new percent\n",
        "    #print(librarytoretain, \"OLD PERCENT:\", libmetadata_dict[librarytoretain][4])\n",
        "    libmetadata_dict[librarytoretain][4] = str(newpercent)  # update percent\n",
        "    #print(\"row[2], NEW PERCENT:\", libmetadata_dict[librarytoretain][4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "_ew4Kxvc4ybD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title def deassign\n",
        "def deassign(reduceby, OCN, groupdf):\n",
        "\n",
        "  # pick which libs get added to daccessdf, allocate N/S has already happened by here, set only contains just north or just south - now looking at tiers\n",
        "\n",
        "  # row: Pandas(Title='Test Title', Author='Zaehner, R. C.', Library='Bard College', _3='703157', ISBN=394718933.0, _5='Vintage Books', LC='BL65', _7='BL', Edition=nan, _9=1974, Language='English', MatchKey='zendrugsandmysticism_______________________________________________________1974____1__vintaa________________________________________zaehn_______________p', _12=nan, OCN=12345, _14=12345, _15=np.int64(7), _16=3, _17=nan, _18='N', HathiTrust=nan, _20=nan, Suspect=nan, _22=nan, _23=nan, Zone='North', _25='RIU,NKF,RRR,VXW,AUM,AMH,WLU ', _26=nan, Region='Mid Atlantic')\n",
        "  assert len(groupdf) >= reduceby, \"Trying to remove more than we have in Assign function\" # something really wrong if this fails - trying to reduce by more than have\n",
        "  global deaccessdf\n",
        "  global tier3total\n",
        "  global totalreduceby # sanity check - did we get all of the reduce by into into this subroutine?\n",
        "\n",
        "  totalreduceby += reduceby\n",
        "\n",
        "  ############# this is an alternative to 'merge' below - faster with small datasets like this, slower with large ones\n",
        "  # choosefrom_df = pd.DataFrame(columns=['Library', 'Percent Category', 'Cohort', 'FEMA Zone'])\n",
        "  # for row in groupdf.itertuples(index=False): # iterate each library that has this ocn\n",
        "  #   choosefrom_df['Library'] = row.Library\n",
        "  #   choosefrom_df['Percent Category'] = libmetadata_dict[row.Library]['Percent Category']\n",
        "  #   choosefrom_df['Cohort'] = libmetadata_dict[row.Library]['Cohort']\n",
        "  #   choosefrom_df['FEMA Zone'] = libmetadata_dict[row.Library]['FEMA Zone']\n",
        "  #   choosefrom_df['Participating'] = libmetadata_dict[row.Library]['Participating']\n",
        "  ##########\n",
        "\n",
        "  # merge groupdf with libmetameta_df - here is the dataset we'll use to pick the deaccession winers\n",
        "  choosefrom_df = groupdf.merge(libmetadata_df, on='Library', how='left')[['Library','Percent Category', 'Cohort', 'FEMA Zone', 'FEMA Random Weight', 'Participating']]\n",
        "  if testing:\n",
        "    print(\"OCN count to start (just N or S here not total OCN count): \", len(groupdf))\n",
        "    print(\"CHOOSE FROM:\\n\" + choosefrom_df['Library'].to_string(index=False, header=False))\n",
        "\n",
        "# Create Tier1 and check if participating\n",
        "  tier1_df = choosefrom_df[\n",
        "      (choosefrom_df['Percent Category'].isin(['High', 'Very High'])) &\n",
        "      (choosefrom_df['Cohort'].isin(['C1', 'C2', 'M-Florida-2019', 'M-2020', 'M-USMAI-2020'])) &\n",
        "      (choosefrom_df['Participating'] != 'No')\n",
        "  ]\n",
        "\n",
        "  if testing:\n",
        "   print(\"Tier 1: \", len(tier1_df), \"Reduceby:\", reduceby, \"\\n\", tier1_df['Library'].to_string(index=False, header=False))\n",
        "\n",
        "  # Real selection work starts here:\n",
        "\n",
        "  if len(tier1_df) == reduceby: # wow, exactly right number of surplus copies\n",
        "    assign_df = tier1_df\n",
        "    if testing:\n",
        "      print(\"Tier1df => Assigned\\n\", assign_df['Library'].to_string(index=False, header=False))\n",
        "    reduceby = reduceby - len(tier1_df) # shouldn't you just say zero?\n",
        "\n",
        "  elif len(tier1_df) > reduceby: # in tier1 there are plenty to chose from for desassign, pick them\n",
        "    #assign = tier1_df.sample(n=reduceby)# randomly pick reduceby number of libraries. .sample gets unique picks, but can't be weighted.\n",
        "    weights = tier1_df['FEMA Random Weight'].values\n",
        "    probabilities = weights / weights.sum() # Normalize weights - np.random.choice needs values that equal 1\n",
        "    assign_df = tier1_df.loc[np.random.choice(tier1_df.index, size=reduceby, replace=False, p=probabilities)]\n",
        "    if testing:\n",
        "      print(\"Tier1df => Assigned\\n\", assign_df['Library'].to_string(index=False, header=False))\n",
        "    reduceby = reduceby - len(assign_df) # should be zero?\n",
        "\n",
        "  else: # need more than tier1 - assign all tier one and make tier 2\n",
        "    assign_df = tier1_df # assign all tier one\n",
        "    reduceby = reduceby - len(tier1_df) # update reduceby\n",
        "\n",
        "    choosefrom_df = choosefrom_df[~choosefrom_df['Library'].isin(tier1_df['Library'])]# remove tier one from choosefrom\n",
        "    if testing:\n",
        "      print(\"...At tier 2\") # make tier 2\n",
        "    tier2_df = choosefrom_df[\n",
        "      (choosefrom_df['Percent Category'].isin(['Medium' or 'Low'])) &\n",
        "      (choosefrom_df['Cohort'].isin(['C1', 'C2', 'M-Florida-2019', 'M-2020', 'M-USMAI-2020'])) &\n",
        "      (choosefrom_df['Participating'] != 'No')\n",
        "    ]\n",
        "    if testing:\n",
        "      print(\"Tier 2: \", len(tier2_df), \"Reduceby:\", reduceby, \"\\n\", tier2_df['Library'].to_string(index=False, header=False))\n",
        "\n",
        "    if len(tier2_df) == reduceby: # Tier2 has just exactly what we need\n",
        "      assign_df = pd.concat([assign_df, tier2_df])\n",
        "      reduceby = reduceby - len(tier2_df) # should be zero\n",
        "    elif len(tier2_df) > reduceby: # in tier2 there are plenty to chose from for desassign, pick them\n",
        "      weights = tier2_df['FEMA Random Weight'].values\n",
        "      probabilities = weights / weights.sum() # Normalize weights - np.random.choice needs values that equal 1\n",
        "      assign_df = pd.concat([assign_df, tier2_df.loc[np.random.choice(tier2_df.index, size=reduceby, replace=False, p=probabilities)]])\n",
        "      reduceby = reduceby - reduceby # why not just say equal 0 ...\n",
        "    else:\n",
        "      # assign all tier 2, make a tier 3, make sure all reduceby allocated, and do you need to reset 'assign' each time?\n",
        "      assign_df = pd.concat([assign_df, tier2_df]) # assign all tier two\n",
        "      reduceby = reduceby - len(tier2_df) # update reduceby\n",
        "\n",
        "      choosefrom_df = choosefrom_df[~choosefrom_df['Library'].isin(tier2_df['Library'])]# remove tier two from choosefrom\n",
        "      if testing:\n",
        "        print(\"BUT WAIT, there's more for Tier 3: \", reduceby, \"Len of choosefrom:\", len(choosefrom_df))\n",
        "      tier3total = tier3total + reduceby # update tier 3 total to see how many copies fall in this bucket\n",
        "\n",
        "      #SEA CHECK THIS/UDATE to right formula as you add more tiers, this basically makes it always work for assert - geesh.\n",
        "      reduceby = reduceby - reduceby\n",
        "\n",
        " # foreach assigned library, print the groupdf['Library'] row to a df\n",
        "  for row in assign_df.itertuples(index=False): # iterate each library assigned and add to deaccess df\n",
        "    subset = groupdf[groupdf[\"Library\"] == row.Library]\n",
        "    deaccess_list.append(subset)  # append to the list instead of growing a df\n",
        "    #deaccessdf = pd.concat([deaccessdf, groupdf[groupdf[\"Library\"] == row.Library]], ignore_index=True) # ineffiecient over time\n",
        "\n",
        "  assert reduceby == 0, f\"REDUCEBY: {reduceby} . Should have been 0\" # definitely something wrong if this isn't zero by here - and you'll end up here until you do the \"but wait\" section above\n",
        "  if testing:\n",
        "    print(\"___\")\n",
        "\n",
        "  return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UZS9jOOX44xr",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title def Allocate\n",
        "def allocate():\n",
        "    count = 0\n",
        "    lnow = datetime.now(tz=ZoneInfo('America/Los_Angeles')) # initialize\n",
        "\n",
        "    global totalsurplusaccordingtoalloc\n",
        "\n",
        "    # Precompute Zone filters once - suggested by ChatGPT for speed\n",
        "    north_mask = df['Zone'] == 'North'\n",
        "    south_mask = df['Zone'] == 'South'\n",
        "\n",
        "    for ocn, group in df.groupby('Current OCN'): # foreach distinct OCN = could alternatively use matchkey here\n",
        "      count += 1\n",
        "      if (count) % feedbackcounter == 0: # start timer\n",
        "        now =  datetime.now(tz=ZoneInfo('America/Los_Angeles'))\n",
        "        elapsed_time = now - start_time\n",
        "        interval_time = now - lnow\n",
        "        #total_minutes = td.total_seconds() // 60  # full minutes including days ; .total_seconds is total, .seconds is in the last day\n",
        "\n",
        "        print(f\"Processed {count} rows, Elapsed seconds: {elapsed_time.total_seconds() /60:.2f}, Interval: {interval_time.total_seconds() /60:.2f}\")\n",
        "        lnow = now\n",
        "\n",
        "      if testing:\n",
        "        print(f\"\\nGroup: {int(ocn)}\") # currently working on this OCN\n",
        "      # figure out how many spare copies there are\n",
        "      numberofcopies = len(group)\n",
        "\n",
        "      # SEA HERE\n",
        "      # this is kinda problematic because this number is based on OCN overlap in GR file, not API overlap.\n",
        "      #numberSurplus = numberofcopies - maxretain\n",
        "      # what would happen if we used an EAST Counts from API number here?\n",
        "      numberSurplus = group['EAST Counts'].iloc[0] - maxretain\n",
        "      # check if numberSurplus greater than numberofcopies - that would be a problem!\n",
        "      # opting to use API when that surplus GT group size, and group size otherwise\n",
        "      if numberSurplus > numberofcopies:\n",
        "          numberSurplus = numberofcopies - maxretain\n",
        "      if testing:\n",
        "          print(\"number of GR copies:\" , numberofcopies, \", API surplus:\", numberSurplus)\n",
        "          #number of copies: 6 , surplus: 1\n",
        "      assert numberofcopies >= numberSurplus, f\"numberSurplus ( usually but not always via API) {numberSurplus} results in a number greater than the GR available group: {numberofcopies} for {ocn}\\n {group} \"\n",
        "\n",
        "\n",
        "\n",
        "      totalsurplusaccordingtoalloc += numberSurplus\n",
        "      if testing:\n",
        "        print(\"totalsurplusaccordingtoalloc:\", totalsurplusaccordingtoalloc)\n",
        "\n",
        "      #numberNorth = (group['Zone'] == 'North').sum()\n",
        "      #numberSouth = (group['Zone'] == 'South').sum()\n",
        "\n",
        "      # Use masks directly for speed - suggested by ChatGPT as replacement for above\n",
        "      numberNorth = north_mask[group.index].sum()\n",
        "      numberSouth = south_mask[group.index].sum()\n",
        "\n",
        "      if testing:\n",
        "        print(\"number of copies:\" , numberofcopies, \", surplus:\", numberSurplus)\n",
        "        print(numberNorth, \"Held in the North\")\n",
        "        print(numberSouth, \"Held in the South\")\n",
        "\n",
        "      #Given 3 variable numberNorth, numberSouth and numberSurplus, determine how many in numberSurplus should be removed from numberNorth and/or numberSouth to make numberNorth and numberSouth be as equal as possible\n",
        "      reduceNorthby, reduceSouthby, surplus_remaining, numberSouth, numberNorth = balance_north_south(numberNorth, numberSouth, numberSurplus, testing)\n",
        "      if testing:\n",
        "        print(\"Remove North: \", reduceNorthby)\n",
        "        print(\"Remove South: \", reduceSouthby)\n",
        "\n",
        "      # de-allocate - pass in the subgroups\n",
        "      if reduceNorthby > 0:\n",
        "        subgroup = group[group['Zone']=='North'] # make a df subgroup of just North\n",
        "        deassign(reduceNorthby, ocn, subgroup)\n",
        "\n",
        "      if reduceSouthby > 0:\n",
        "        subgroup = group[group['Zone']=='South'] # make a df subgroup of just South\n",
        "        deassign(reduceSouthby, ocn, subgroup)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Vemf01WBKT2c",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title SKIPPING define dictionaries for dataframes - might not be using libmetadatadict - this might be unncecessary\n",
        "%%skip\n",
        "libmetadata_dict = {} # map name to metadata\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Bmayx72ipyn5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9671f3e-bbbe-4382-9239-286eda3002de",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/5+/InputFiles/ForALLFile_EAST5+.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-194557436.py:9: DtypeWarning: Columns (13,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(infile, dtype=column_types)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df w/ header rows:  3017291\n",
            "df w/o header rows: 3017291\n",
            "df deduped len: 2998251\n"
          ]
        }
      ],
      "source": [
        "#@title Read in intial data, drop header rows, create empty deaccess_list, SET GLOBALS\n",
        "print(infile)\n",
        "# Define the column types in a dictionary\n",
        "column_types = {\n",
        "    'Control Number(001)': 'string',  # For string data\n",
        "    #'EAST Counts': 'Int64'  # fails if it hits a header row, deleting those below\n",
        "    'EAST Counts': 'string'\n",
        "}\n",
        "df = pd.read_csv(infile, dtype=column_types)\n",
        "#print(df.columns)\n",
        "\n",
        "print(\"df w/ header rows: \", len(df))\n",
        "Headers_to_drop = df[df['Library'].str.startswith('Library')].index\n",
        "df.drop(index=Headers_to_drop, inplace=True)\n",
        "print(\"df w/o header rows:\", len(df))\n",
        "\n",
        "# EAST counts is #.# or ?? and needs to be int\n",
        "df['EAST Counts'] = pd.to_numeric(df['EAST Counts'], errors='coerce')\n",
        "df['EAST Counts'] = df['EAST Counts'].round().astype('Int64')\n",
        "\n",
        "df['Control Number(001)'] = df['Control Number(001)'].astype('string')\n",
        "\n",
        "'''\n",
        "is_duplicate = df.duplicated(subset=[\"Library\", \"OCN\"])\n",
        "removed_rows = df[is_duplicate]\n",
        "print(\"removed rows:\", len(removed_rows), removed_rows[[\"Library\", \"OCN\"]])\n",
        "'''\n",
        "df = df[~df.duplicated(subset=[\"Library\", \"OCN\"])]\n",
        "\n",
        "print(\"df deduped len:\", len(df))\n",
        "\n",
        "deaccess_list = []  # global deaccessdf with this list\n",
        "tier3total = 0 # global tier 3 count - this eventually will not be necessary\n",
        "totalreduceby = 0 # just another counter for sanity\n",
        "totalsurplusaccordingtoalloc = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_Y6T0uIv8M11",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title SKIPPING Read in field definitions, libmetadata, make states_dict - DON\"T THINK I NEED THIS SECTION, except maybe fields\n",
        "%%skip\n",
        "# 0:instID, 1:lib, 2:GRLibName, 3:State, 4:Sym, 5:CollSize, 6:Retentions, 7:%, 8:Type, 9:Zone, 10:Region\n",
        "fields_df = pd.read_excel(fields, sheet_name='Field Definitions 5+', dtype=str) ## Read in field definitions\n",
        "# states_df = pd.read_excel(libmetadata, sheet_name='Zones', dtype=str) ## Read in state to zone mappings\n",
        "# states_dict = states_df.set_index('State')['Zone'].to_dict() # Make states to zone dict\n",
        "# risk_df = pd.read_excel(libmetadata, sheet_name='Risk', dtype=str) ## Read in risk definitions\n",
        "# risk_dict = risk_df.set_index('Library')['FEMA ZONE'].to_dict() # Make id to risk\n",
        "\n",
        "# if testing:\n",
        "#   print(risk_dict)\n",
        "#   print(states_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dieVA-rKKlIO"
      },
      "outputs": [],
      "source": [
        "#@title Read the EAST Libraries Metadata file, create Low/Medium/High/Very High column\n",
        "#  I think I have since added a column with low/med etc. in data file, but this does the same...\n",
        "\n",
        "libmetadata_df = pd.read_excel(libmetadata, sheet_name='Plus5', dtype=str)\n",
        "libmetadata_df['# in Gold Rush (2022)'] = pd.to_numeric(libmetadata_df['# in Gold Rush (2022)'], errors='coerce').astype(pd.Int64Dtype()) # make these ints\n",
        "libmetadata_df['Est Collection Size'] = pd.to_numeric(libmetadata_df['Est Collection Size'], errors='coerce').astype(pd.Int64Dtype())\n",
        "libmetadata_df['FEMA Random Weight'] =  pd.to_numeric(libmetadata_df['FEMA Random Weight'],  errors='coerce').astype(pd.Int64Dtype())\n",
        "\n",
        "\n",
        "\n",
        "libmetadata_df[\"Percent Collection\"] = pd.to_numeric(libmetadata_df[\"Percent Collection\"], errors=\"coerce\") * 100\n",
        "\n",
        "conditions = [\n",
        "    libmetadata_df[\"Percent Collection\"] < 10,\n",
        "    libmetadata_df[\"Percent Collection\"].between(10, 20, inclusive=\"both\"), # ends at 20\n",
        "    libmetadata_df[\"Percent Collection\"].between(20, 25, inclusive=\"both\"), # starts at 20.00001 sort of\n",
        "    libmetadata_df[\"Percent Collection\"] > 25\n",
        "]\n",
        "\n",
        "# Define the corresponding values\n",
        "choices = [\"Low\", \"Medium\", \"High\", \"Very High\"]\n",
        "\n",
        "# Apply conditions\n",
        "libmetadata_df[\"Percent Category\"] = np.select(conditions, choices, default=\"Unknown\")\n",
        "\n",
        "#print(libmetadata_df[['Name in Gold Rush Reports EAST', 'Percent Collection', 'Percent Category']].to_string(index=False))\n",
        "#print(libmetadata_df[['Percent Category']].to_string(index=False))\n",
        "\n",
        "libmetadata_dict = libmetadata_df.set_index('Name in Gold Rush Reports EAST').to_dict(orient='index') # not sure I'm using this anymore?\n",
        "\n",
        "libmetadata_df = libmetadata_df.rename(columns={\"Name in Gold Rush Reports EAST\": \"Library\"})\n",
        "\n",
        "#print(libmetadata_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "cellView": "form",
        "id": "s6VnorXGFWMM"
      },
      "outputs": [],
      "source": [
        "#@title SKIP Read in and make df for noocnworksheet\n",
        "%%skip\n",
        "column_types = {\n",
        "    'Control Number(001)': 'string',  # For string data\n",
        "}\n",
        "noOCN_df = pd.read_csv(infilenoocn, dtype=column_types) # read the no OCLC numbers infile and make a pandas dataframe - noOCN_df\n",
        "\n",
        "total_noOCN = len(noOCN_df)\n",
        "\n",
        "noOCN_df.rename(columns={'OCN': 'OCLC Number', 'States Retained In': 'Retained In State'}, inplace=True)\n",
        "\n",
        "# another approach would be -> for lib in noOCN_df['Library'].unique():\n",
        "\n",
        "groups = noOCN_df.groupby('Library')\n",
        "for lib, row in groups:\n",
        "  if lib.startswith('SC') or lib.startswith('Library'):\n",
        "    continue\n",
        "\n",
        "  #print(\"Name:\", lib, \"Row:\", row)\n",
        "  #print(type(row))\n",
        "  #(row['Title'])\n",
        "  #print(row['Library'])\n",
        "\n",
        "  #print(row.iloc[:, [2,3,13,0,1,4,5,9,8,6,7,10,12]])\n",
        "  rows_to_append = row.iloc[:, [2,3,13,0,1,4,5,9,8,6,7,10,12]]  #\n",
        "  #print(type(rows_to_append)) #df\n",
        "  #print( type( dataframes_NoOCN[lib])) #df\n",
        "\n",
        "  #existing_df = pd.concat([existing_df, rows_to_append], ignore_index=True)\n",
        "  #dataframes_NoOCN['Anderson University']\n",
        "  dataframes_NoOCN[lib] = pd.concat([dataframes_NoOCN[lib], rows_to_append], ignore_index=True)\n",
        "\n",
        "\n",
        "  #dataframes_NoOCN[row['Library']].loc[len(dataframes_NoOCN[row['Library']].index)] = rows_to_append.tolist()\n",
        "  libmetadata_dict[lib][10] += len(rows_to_append)\n",
        "  #libmetadata_dict[lib][10] += 1 # increment noOCN\n",
        "\n",
        "\n",
        "print(\"items in noocn csv:\", len(noOCN_df))\n",
        "print(\"libraries in dataframes_NoOCN:\", len(dataframes_NoOCN))\n",
        "\n",
        "#print(len(dataframes_NoOCN['Bard College']))\n",
        "#print((dataframes_NoOCN['Bard College']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "1s3-50tjXXbq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81236393-f973-40e0-a860-4cf4374981b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of DF before drop EAST < 6: 2998251\n",
            "length of DF after drop  EAST < 6: 1515470\n"
          ]
        }
      ],
      "source": [
        "#@title reduce df to only EAST > 5 (drop less than 6)\n",
        "\n",
        "\n",
        "print(\"length of DF before drop EAST < 6:\", len(df))\n",
        "to_drop = df[ (df['EAST Counts'] < 6)  | (df['EAST Counts'].isna()) ].index  # drop if EAST < 6 or is NaNprint(len(to_drop))\n",
        "df.drop(index=to_drop, inplace=True)\n",
        "print(\"length of DF after drop  EAST < 6:\", len(df))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "5F_QH8x0R4Xl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd79e352-f37b-43ee-9e9e-f37caa34530b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting to allocate multiple copies: 2025-08-08 17:03:14\n",
            "Number of OCNS needing allocation: 236101\n",
            "Processed 50000 rows, Elapsed seconds: 6.61, Interval: 5.71\n",
            "Processed 100000 rows, Elapsed seconds: 12.11, Interval: 5.50\n",
            "Processed 150000 rows, Elapsed seconds: 17.69, Interval: 5.58\n",
            "Processed 200000 rows, Elapsed seconds: 23.13, Interval: 5.44\n"
          ]
        }
      ],
      "source": [
        "#@title Call allocate()\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "print(\"Starting to allocate multiple copies:\", datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S')) # Convert to a datetime object\n",
        "print(\"Number of OCNS needing allocation:\", df['Current OCN'].nunique())\n",
        "if testing:\n",
        "    print(df['Current OCN'].dropna().unique())\n",
        "#allocate(df, libmetadata_dict)\n",
        "allocate()\n",
        "#------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "E2KIuk9mgXpV"
      },
      "outputs": [],
      "source": [
        "#@title Make spreadsheets per lib -\n",
        "#print(deaccess_list[0]) # deaccess list is a list of dataframes - might want to rethink that...\n",
        "#print(f\"Type of first element: {type(deaccess_list[0])}\")\n",
        "\n",
        "deaccess_df = pd.concat(deaccess_list, ignore_index=True) # concat all the dfs into one\n",
        "\n",
        "print_columns = ['Library', 'Control Number(001)', 'Title', 'Author', 'ISBN', 'Publisher Name', 'LC', 'LC Class', 'Edition', 'Publication Date', 'Language', '035 field', 'OCN', 'Current OCN']\n",
        "\n",
        "deaccess_df.to_csv(outputallfile, index=False)\n",
        "\n",
        "for library, group_df in deaccess_df.groupby('Library'):\n",
        "    #print(f\"Library: {library}\")\n",
        "    outputfile = outputdir + library + \".xlsx\"\n",
        "\n",
        "    with pd.ExcelWriter(outputfile, engine='xlsxwriter') as writer: # create a excel writer object\n",
        "        workbook  = writer.book\n",
        "        comma_format = workbook.add_format({'num_format': '[=0]0;[>0]#,###'}) # thanks claude\n",
        "        text_format = workbook.add_format({'num_format': '@'})\n",
        "\n",
        "        group_df[print_columns].to_excel(writer, sheet_name=\"Deassigned\", index=False, header=True, freeze_panes=(1,0)) #\n",
        "\n",
        "        writer.sheets['Deassigned'].set_column('A:B', 20, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('C:C', 40, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('D:D', 30, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('E:E', 15, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('F:F', 30, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('G:G', 10, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('H:H', 6, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('I:I', 12, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('J:J', 12, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('K:K', 12, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('L:L', 20, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('M:M', 12, text_format)\n",
        "        writer.sheets['Deassigned'].set_column('N:N', 12, text_format)\n",
        "\n",
        "        # Define a format for wrapped text\n",
        "        header_format = workbook.add_format({'text_wrap': True, 'bold': True, 'bg_color': 'silver'}) # silver is #C0C0C0\n",
        "        for col_num, value in enumerate(group_df[print_columns].columns.values):         # Apply the format to the header row\n",
        "            writer.sheets['Deassigned'].write(0, col_num, value, header_format)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Make summary counts sheet\n",
        "\n",
        "summary_file = outputdir + \"Summary.xlsx\"\n",
        "\n",
        "deaccessdf = pd.concat(deaccess_list, ignore_index=True)\n",
        "\n",
        "summary_counts_df = deaccessdf['Library'].value_counts().reset_index()\n",
        "summary_counts_df.columns = ['Library', 'Counts']\n",
        "summary_counts_df = summary_counts_df.sort_values(by=\"Library\").reset_index(drop=True)\n",
        "print(summary_counts_df)\n",
        "\n",
        "deaccessdf.to_csv(outputallfile, index=False) # write out fill results file\n",
        "\n",
        "with pd.ExcelWriter(summary_file, engine='xlsxwriter') as writer2:\n",
        "    summary_counts_df.to_excel(writer2, sheet_name=\"Summary Counts\", index=False, header=True, freeze_panes=(1,0))\n",
        "    workbook2  = writer2.book\n",
        "\n",
        "    countsworksheet2 = writer2.sheets['Summary Counts']\n",
        "    countsworksheet2.set_column('A:A', 30)  # Column A:A width\n",
        "    countsworksheet2.set_column('B:F', 10)  # Column B:F width\n"
      ],
      "metadata": {
        "id": "QCuHCeNUjHpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f4d8763-a91d-405a-a071-47e86f070aa7",
        "cellView": "form"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                             Library  Counts\n",
            "0                    Adelphi Swirbul   23607\n",
            "1                    Amherst College   11673\n",
            "2                       Bard College     763\n",
            "3                     Boston College   16739\n",
            "4                  Boston University    2997\n",
            "5                           Brandeis   22812\n",
            "6            Bridgewater State Univ.   10847\n",
            "7                  Bryn Mawr College    7961\n",
            "8                Bucknell University    1209\n",
            "9                      Colby College     120\n",
            "10               Connecticut College    4875\n",
            "11                  Davidson College     489\n",
            "12                DeSales University     304\n",
            "13              Fairfield University   10726\n",
            "14       Florida Atlantic University     533\n",
            "15     Florida Gulf Coast University     271\n",
            "16  Florida International University     592\n",
            "17          Florida State University     490\n",
            "18             Frostburg State Univ.    1021\n",
            "19                Gettysburg College     851\n",
            "20          Hamilton College Library   29666\n",
            "21                 Hampshire College    4175\n",
            "22                 Lafayette College   14936\n",
            "23                 Loyola Notre Dame    1861\n",
            "24                Middlebury College    7929\n",
            "25           Morgan State University     650\n",
            "26                Mt Holyoke College    7795\n",
            "27               New York University    1474\n",
            "28           Phillips Exeter Academy    6308\n",
            "29              Saint Anselm College    9933\n",
            "30              Salisbury University     575\n",
            "31                  Skidmore College   10143\n",
            "32                     Smith College    8232\n",
            "33          St. Mary's College of MD     660\n",
            "34                Swarthmore College   13732\n",
            "35               Syracuse University    4460\n",
            "36                 Towson University    1720\n",
            "37                   Trinity College   12225\n",
            "38                  Tufts University    8601\n",
            "39              UMD Baltimore County    2529\n",
            "40                 UMD Eastern Shore     725\n",
            "41                     UMass Amherst   68765\n",
            "42                      UMass Boston    8608\n",
            "43                   UMass Dartmouth    9980\n",
            "44                      UMass Lowell   12699\n",
            "45                     Union College    1652\n",
            "46             Univ of New Hampshire   12245\n",
            "47           University of Baltimore     226\n",
            "48           University of Rochester   16002\n",
            "49                    Vassar College    2243\n",
            "50                 Wellesley College   12988\n",
            "51               Wesleyan University    8592\n",
            "52                  Williams College   10261\n",
            "53                Yeshiva University     919\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "jkTb_5LpQnQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88ee8d6d-c7be-499d-cf67-7a4f8a66a7db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Summary---\n",
            " EAST  Copies  EAST Counts Minus 5  Surplus\n",
            "    6  659760                    1   109960\n",
            "    7  426493                    2   121855\n",
            "    8  235280                    3    88230\n",
            "    9  116104                    4    51601\n",
            "   10   52761                    5    26380\n",
            "   11   19352                    6    10555\n",
            "   12    4539                    7     2647\n",
            "   13     963                    8      592\n",
            "   14     173                    9      111\n",
            "   15      31                   10       20\n",
            "   16      14                   11        9\n",
            "Overall, 236101 titles representing 1515470 copies had more than 5 copies\n",
            "\n",
            "Total number of surplus copies according to countsdf  : 411960 <= will be wrong if some OCN/Matchkey count mismatches\n",
            "Total number of surplus copies according to allocation: 440406\n",
            "Total number that went to deaccession - should be same as surplus: 441448\n",
            "\n",
            "Len deaccessdf: 432389 should be the same as surplus copies plus tier 3: 9199\n",
            "TIER 3 total - not assigning 9199\n",
            "\n",
            "Started at: 2025-08-08 10:02:20\n",
            "Ending at:  2025-08-08 10:36:21\n",
            "Ran for minutes: 34.03\n"
          ]
        }
      ],
      "source": [
        "#@title summary to screen\n",
        "print(\"----Summary---\")\n",
        "\n",
        "# quick summary of how many surplus copies\n",
        "# (Overlap Level MINUS required number to keep) * Number of MatchKeys or OCNS at that level\n",
        "# need for calculations:Overlap Level,\tUnique Matchkeys or OCNS at that level =>\tNumber of Copies\n",
        "\n",
        "counts_df = df['EAST Counts'].value_counts().reset_index()\n",
        "counts_df.columns = ['EAST', 'Copies']\n",
        "counts_df['EAST Counts Minus 5'] = counts_df['EAST'] - 5\n",
        "counts_df['Surplus'] = counts_df['EAST Counts Minus 5'] * (counts_df['Copies']/counts_df['EAST'])\n",
        "counts_df['Surplus'] = counts_df['Surplus'].astype(int)\n",
        "\n",
        "#print(counts_df.sort_values(by='EAST'))\n",
        "print(counts_df.sort_values(by='EAST').to_string(index=False)) # don't include index\n",
        "\n",
        "# could work for matchkey count too\n",
        "print('Overall,', df['Current OCN'].nunique(), \"titles representing\", len(df), \"copies had more than 5 copies\")\n",
        "print(\"\\nTotal number of surplus copies according to countsdf  :\", counts_df['Surplus'].sum(), \"<= will be wrong if some OCN/Matchkey count mismatches\")\n",
        "print(\"Total number of surplus copies according to allocation:\", totalsurplusaccordingtoalloc)\n",
        "print(\"Total number that went to deaccession - should be same as surplus:\", totalreduceby)\n",
        "print(\"\\nLen deaccessdf:\", len(deaccessdf), \"should be the same as surplus copies plus tier 3:\", tier3total)\n",
        "\n",
        "print(\"TIER 3 total - not assigning\", tier3total)\n",
        "\n",
        "\n",
        "# End the timer and report how long it took\n",
        "end_time = datetime.now(tz=ZoneInfo('America/Los_Angeles'))\n",
        "print(f\"\\nStarted at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "print(f\"Ending at:  {end_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "elapsed_time = end_time - start_time\n",
        "print(f\"Ran for minutes: {elapsed_time.total_seconds() / 60:.2f}\")\n",
        "\n",
        "#print(f\"Script took {elapsed_time.resolution}  to run.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1F7VPQRI8XdZOSaLD6VJIQQwlSxFXLsmh",
      "authorship_tag": "ABX9TyMqAxNQwcxKFYvqXHCkAZuE",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}